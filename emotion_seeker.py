# -*- coding: utf-8 -*-
"""Emotion Seeker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lEECwOGIaH6fUV58OMz_E4dgAabAfHXn
"""

import math
from collections import defaultdict

class NaiveBayes():
  def __init__(self, train_file, val_file, test_file):
    self.train_X, self.train_y = self.load_data(train_file)
    self.val_X, self.val_y = self.load_data(val_file)
    self.test_X = self.load_test_data(test_file)
    self.v = 0
    self.sum_word = 0

  def load_data(self, file_path):
    X = []
    y = []
    with open(file_path, 'r') as f:
      for line in f:
        sentence, emotion = line.strip().split(';')
        X.append(sentence.split())  # Split sentence into words
        y.append(emotion)
    return X, y

  def load_test_data(self, file_path):
    X = []
    with open(file_path, 'r') as f:
      for line in f:
        sentence = line.strip()
        X.append(sentence.split())  # Split sentence into words
    return X

  def _get_prob_smoothing(self, c, word):
    probability = (self.wc_count[word][c] + 1) / (self.sum_word + self.v)
    return probability

  def train(self):
    self.c_count = defaultdict(lambda: 0)
    for label in self.train_y:
      self.c_count[label] += 1
    self.c_prob = {label: count / len(self.train_y) for label, count in self.c_count.items()}
    self.c_type = len(self.c_count)

    train_X_BOW = set()
    for x in self.train_X:
      for word in x:
        train_X_BOW.add(word)
    self.sum_word = len(train_X_BOW)

    self.wc_count = defaultdict(lambda: defaultdict(lambda: 0))
    for word in train_X_BOW:
      self.v += 1
      for i, sent in enumerate(self.train_X):
        if word in sent:
          self.wc_count[word][self.train_y[i]] += 1

    self.wc_prob = defaultdict(lambda: defaultdict(lambda: 0))
    for word in train_X_BOW:
      for label in self.c_count.keys():
        self.wc_prob[word][label] = self._get_prob_smoothing(label, word)

  def classify(self, sent):
    wclog_sum = defaultdict(lambda: 0)
    for label in self.c_count.keys():
      for word in sent:
        wclog_sum[label] += math.log(self._get_prob_smoothing(label, word))

    cdlog_prob = defaultdict(lambda: 0)
    for label in self.c_count.keys():
      cdlog_prob[label] = wclog_sum[label] + math.log(self.c_prob[label])
    return max(cdlog_prob, key=cdlog_prob.get)

  def test(self):
    for sent in self.test_X:
      predicted_label = self.classify(sent)
      print(' '.join(sent), ": Predicted:", predicted_label)

  def Test(self, test_file):
    correct_predictions = 0
    total_predictions = 0
    with open(test_file, 'r') as f:
      for line in f:
        sentence, true_label = line.strip().split(';')
        predicted_label = self.classify(sentence.split())
        if predicted_label == true_label:
          correct_predictions += 1
        total_predictions += 1
      accuracy = correct_predictions / total_predictions
      print("Accuracy:", accuracy)

if __name__ == '__main__':
    train_file = 'train.txt'
    val_file = 'val.txt'
    test_file = 'test.txt'
    Test_file = 'Test.txt'
    nb = NaiveBayes(train_file, val_file, test_file)
    nb.train()
    nb.test()
    nb.Test(Test_file)