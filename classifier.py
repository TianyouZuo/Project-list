# -*- coding: utf-8 -*-
"""Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxtS9UKnWIw3ayHitWI09W3uDjRlpP7u
"""

import pandas as pd
import numpy as np

"""## **Lasso Classifier**"""

#Set seed
np.random.seed(42)

# Read csv
df = pd.read_csv('wine.csv')

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.metrics import median_absolute_error, mean_absolute_error
from sklearn.compose import make_column_selector as selector

float_pipeline = Pipeline(
    steps=[
        ("imputation_knn", KNNImputer(n_neighbors = 5, weights = 'distance',add_indicator = False, copy = False)),
        ("scaler", MinMaxScaler()),
    ]
)

# This pipeline is for the binary categorical features that don't require one-hot encoding
object_pipeline = Pipeline(
    steps=[
        ("imputation_knn", KNNImputer(n_neighbors = 5, weights = 'distance',add_indicator = False, copy = False)),
    ]
)

catg_pipeline = Pipeline(
    steps=[
        ("imputation_mode",  SimpleImputer(strategy= 'most_frequent')), #knn doesn't work for one hot encoded
        ('onehotencoder',OneHotEncoder(handle_unknown="ignore")),
    ]
)

column_transformer =  ColumnTransformer(
       transformers = [
        ('float_transformer', float_pipeline, selector(dtype_include="float")),
        ('object_transformer',object_pipeline, selector(dtype_include="object")),
        ('catg_transformer',catg_pipeline, selector(dtype_include="category"))
    ],
)
from sklearn.model_selection import train_test_split,GridSearchCV
X = df.drop(columns = 'Quality')
y = df['Quality']
print(X.shape, y.shape)
# Spilt train and test data
X_train = X.iloc[:30]
X_test = X.iloc[30:]
y_train = y[:30]
y_test = y[30:]

from sklearn.linear_model import Lasso
from sklearn import set_config
set_config(display="diagram")
lasso_pipe = Pipeline(
    steps = [
        ("Preprocessing", column_transformer),
        ("LASSO", Lasso())
    ]
)
# Set parameter range
lasso_params = {'LASSO__alpha':(np.logspace(-1, 0, 3)),
              }
lasso_pipe

# Set 5 fold and scoring
modelLasso=GridSearchCV(lasso_pipe, param_grid = lasso_params, scoring = 'neg_median_absolute_error', cv=5, refit = True)
modelLasso.fit(X_train, y_train)

#Print best model
best_model = modelLasso.best_estimator_
print(best_model)

#Report features ,coefficients and Eout of the best model
features = X_train.columns[best_model.named_steps.LASSO.coef_ != 0]
coefficients = best_model.named_steps.LASSO.coef_
Eout = median_absolute_error(y_test, best_model.predict(X_test))

print(f"Selected features: {', '.join(features)}")
print(f"Coefficients: {coefficients}")
print(f"Eout: {Eout}")

# Debias the model
best_alpha = best_model.named_steps.LASSO.alpha
debias_pipe = Pipeline(
    steps = [
        ("Preprocessing", column_transformer),
        ("LASSO", Lasso(best_alpha))
    ]
)
debias_pipe.fit(X_train[features], y_train)
debias_coef = debias_pipe.named_steps.LASSO.coef_
print(f"Coefficients for debias: {debias_coef}")
debias_Eout = median_absolute_error(y_test, debias_pipe.predict(X_test))
print(f"Eout for debias: {debias_Eout}")

import plotly.express as px

fig = px.scatter( x= y_test, y= best_model.predict(X_test), trendline = 'ols')
fig.update_layout(xaxis_title = 'Actual Value', yaxis_title = 'Predicted Value', title = 'Performance on the test')
fig.show()

"""## **Naive Bayes Classifier**"""

train_data = pd.read_csv('mnist_train.csv', header = None)
test_data = pd.read_csv('mnist_test.csv', header = None)

# Spilt label and pixel data
label_train = train_data.iloc[:, 0]
pixel_train = train_data.iloc[:, 1:]
label_test = test_data.iloc[:, 0]
pixel_test = test_data.iloc[:, 1:]

# Convert pixel data to binary indicators
binary_train = np.where(pixel_train < 255/2, 0, 1)
binary_test = np.where(pixel_test < 255/2, 0, 1)

# Estimate priors
class_counts = np.bincount(label_train.astype(int))
total_samples = len(label_train)
priors = class_counts / total_samples

print("Class\tP(Class)")
for i, prior in enumerate(priors):
    print(f"{i}:\t{prior:.3f}")

# Function to estimate likelihoods with Laplace smoothing
def estimate_likelihoods(train_labels, train_pixels, k):
    num_classes = 10
    num_pixels = train_pixels.shape[1]
    likelihoods = np.zeros((num_classes, num_pixels, 2))  # 2 for binary features
    V = 2 # 2 possible values

    for digit_class in range(num_classes):
        class_pixels = train_pixels[train_labels == digit_class]
        class_total = class_pixels.shape[0]
        for pixel_idx in range(num_pixels):
            num_ones = class_pixels[:, pixel_idx].sum() + k
            num_zeros = class_total - num_ones + k
            total = class_total + k * V
            likelihoods[digit_class, pixel_idx, 1] = num_ones / total
            likelihoods[digit_class, pixel_idx, 0] = num_zeros / total

    return likelihoods

# Estimate likelihoods with Laplace smoothing for k=1 and k=5
likelihoods_k1 = estimate_likelihoods(label_train, binary_train, 1)
likelihoods_k5 = estimate_likelihoods(label_train, binary_train, 5)

# Extracting specific likelihoods for k=1 and k=5
p_f682_0_class5_k1 = likelihoods_k1[5, 682, 0]
p_f682_0_class5_k5 = likelihoods_k5[5, 682, 0]
p_f772_1_class9_k1 = likelihoods_k1[9, 772, 1]
p_f772_1_class9_k5 = likelihoods_k5[9, 772, 1]

# Print the requested values
print("For k=1:")
print(f"P(F682 = 0|class = 5): {p_f682_0_class5_k1:.3f}")
print(f"P(F772 = 1|class = 9): {p_f772_1_class9_k1:.3f}")

print("\nFor k=5:")
print(f"P(F682 = 0|class = 5): {p_f682_0_class5_k5:.3f}")
print(f"P(F772 = 1|class = 9): {p_f772_1_class9_k5:.3f}")

# Define function to perform MAP classification
def map_classification(test_pixels, priors, likelihoods):
    num_test = test_pixels.shape[0]
    num_classes = 10
    decision_values = np.zeros((num_test, num_classes))

    for i in range(num_test):
      for digit_class in range(num_classes):
        log_prob = np.log(priors[digit_class])
        for pixel_idx in range(test_pixels.shape[1]):
          pixel_value = test_pixels[i, pixel_idx]
          log_likelihood = np.log(likelihoods[digit_class, pixel_idx, pixel_value])
          log_prob += log_likelihood
        decision_values[i, digit_class] = log_prob

    return decision_values
# Perform MAP classification for k=1 and k=5
decision_values_k1 = map_classification(binary_test, priors, likelihoods_k1)
decision_values_k5 = map_classification(binary_test, priors, likelihoods_k5)

# Find the log posterior probability for class 5 and class 7 for the first test image
log_posterior_class5_k1 = decision_values_k1[0, 5]
log_posterior_class5_k5 = decision_values_k5[0, 5]
log_posterior_class7_k1 = decision_values_k1[0, 7]
log_posterior_class7_k5 = decision_values_k5[0, 7]

# Print the results
print("For k=1:")
print(f"P(class = 5): {log_posterior_class5_k1:.3f}")
print(f"P(class = 7): {log_posterior_class7_k1:.3f}")

print("\nFor k=5:")
print(f"P(class = 5): {log_posterior_class5_k5:.3f}")
print(f"P(class = 7): {log_posterior_class7_k5:.3f}")

# Function to perform MAP classification and calculate accuracy for a given value of k
def map_classification_accuracy(test_pixels, true_labels, priors, likelihoods, k):
    num_test = test_pixels.shape[0]
    num_classes = 10
    correct_predictions = 0

    # Perform MAP classification
    decision_values = np.zeros((num_test, num_classes))
    for i in range(num_test):
        for digit_class in range(num_classes):
            log_prob = np.log(priors[digit_class])
            for pixel_idx in range(test_pixels.shape[1]):
                pixel_value = test_pixels[i, pixel_idx]
                log_prob += np.log(likelihoods[digit_class, pixel_idx, pixel_value])
            decision_values[i, digit_class] = log_prob

    # Determine predicted labels
    predicted_labels = np.argmax(decision_values, axis=1)

    # Count correct predictions
    for i in range(num_test):
        if predicted_labels[i] == true_labels[i]:
            correct_predictions += 1

    # Calculate accuracy
    accuracy = (correct_predictions / num_test) * 100
    return accuracy

# Calculate accuracy for each value of k from 1 to 5
accuracies = []
for k in range(1, 6):
    accuracy = map_classification_accuracy(binary_test, label_test, priors, likelihoods_k1, k)
    accuracies.append(accuracy)
    print(f"Accuracy for k={k}: {accuracy:.2f}%")

accuracies